{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network\n",
    "\n",
    "This module provides implementation of a neural network via nested classes.  The neural network class consists of multiple Layer objects, each of which contains parameters pertaining to that layer (e.g., W, b, activation function, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a single layer in a neural network\n",
    "    \n",
    "    Stored parameters:\n",
    "        W = matrix of weights\n",
    "        b = vector of biases\n",
    "        activation = name of activation function for this layer (either 'relu' or 'sigmoid')\n",
    "        Z, A = matrices of outputs, initialized to None and are updated whenever forward_prop\n",
    "            is run\n",
    "        dW, db = partial derivatives of loss with respec to W and b of this layer.\n",
    "            Initialized to None, updated whenever backward_prop is run\n",
    "            \n",
    "    To use, first initialize the layer with appropriate parameters, then run forward_prop,\n",
    "        backward_prop, and update_params\n",
    "        \n",
    "    All private functions are only needed for internal use\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of acceptable activations\n",
    "    activations = ['relu','sigmoid']\n",
    "    \n",
    "    def __init__(self, n_curr, n_prev, activation):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        n_curr = number of nodes in this layer\n",
    "        n_prev = number of nodes in previous layer\n",
    "        activation = name of activation function, must be either 'relu' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.W = np.random.randn(n_curr,n_prev) * 0.01\n",
    "        self.b = np.zeros((n_curr,1))\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        if activation not in self.activations:\n",
    "            raise Exception('invalid activation function name')\n",
    "        else:\n",
    "            self.activation = activation\n",
    "        \n",
    "        assert (self.W.shape == (n_curr, n_prev))\n",
    "        assert(self.b.shape == (n_curr, 1))\n",
    "        \n",
    "    def forward_prop(self, A_prev):\n",
    "        \"\"\"\n",
    "        Calculate Z and A.  These are stored internally and then A is returned\n",
    "        \n",
    "        Arguments:\n",
    "        A_prev = output of layer before this one (e.g., output of layer 2 if this is layer 3)\n",
    "        \n",
    "        Returns:\n",
    "        A\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Z = self._linear_fwd(A_prev)\n",
    "        self.A = self._act_fwd(self.Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward_prop(self, dA, A_prev):\n",
    "        \"\"\"\n",
    "        Calculate partial derivates dW, db, and dA_prev (partial derivative with respect to\n",
    "            output of previous layer). dW and db are stored internally, dA_prev is returned\n",
    "        \n",
    "        Arguments:\n",
    "        dA = partial derivative of loss with respect to activation output of this layer\n",
    "        A_prev = output of layer before this one (e.g., output of layer 2 if this is layer 3)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev (partial derivative with respect to output of previous layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        dZ = self._act_bwd(dA)\n",
    "        self.dW, self.db, dA_prev = self._linear_bwd(dZ, A_prev)\n",
    "        \n",
    "        return dA_prev\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates parameters (W and b) in this layer according to learning_rate specified and\n",
    "            stored values of W, b, dW, and db\n",
    "        \n",
    "        Arguments:\n",
    "        learning_rate\n",
    "        \n",
    "        Returns: none (updated parameters are stored internally)\n",
    "        \"\"\"\n",
    "        self.W = self.W - (learning_rate * self.dW)\n",
    "        self.b = self.b - (learning_rate * self.db)\n",
    "        \n",
    "    def _linear_fwd(self, A_prev):\n",
    "        \"\"\"\n",
    "        Private method for internal use only.\n",
    "        Calculates Z using this layer's parameters (W, b) and output of previous layer\n",
    "        \n",
    "        Arguments:\n",
    "        A_prev = output of previous layer (e.g., of layer 2 if this is layer 3)\n",
    "        \n",
    "        Returns:\n",
    "        Z\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = np.dot(self.W,A_prev) + self.b\n",
    "        assert(Z.shape == (self.W.shape[0],A_prev.shape[1]))\n",
    "        return Z\n",
    "    \n",
    "    def _act_fwd(self, Z):\n",
    "        \"\"\"\n",
    "        Private method for internal use only.\n",
    "        Calculates final output of this layer using appropriate activation function and\n",
    "            supplied linear output Z\n",
    "        \n",
    "        Arguments:\n",
    "        Z = linear output of this layer\n",
    "        \n",
    "        Returns:\n",
    "        A (final output of layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            A = Layer._relu(self.Z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            A = Layer._sigmoid(self.Z)\n",
    "            \n",
    "        assert(A.shape == Z.shape)\n",
    "        return A\n",
    "    \n",
    "    def _linear_bwd(self, dZ, A_prev):\n",
    "        \"\"\"\n",
    "        Private method for internal use only.\n",
    "        \n",
    "        Calculates partial derivatives of loss with respect to this layer's parameters (dW, db)\n",
    "            and previous layer's output (dA_prev) given partial derivative of loss with respect\n",
    "            to linear output of this layer (dZ), and final output of previous layer (A_prev)\n",
    "        \n",
    "        Arguments:\n",
    "        dZ = partial derivative of loss with respect to linear output of this layer\n",
    "        A_prev = output of previous layer (e.g., of layer 2 if this is layer 3)\n",
    "        \n",
    "        Returns:\n",
    "        dW, db, dA_prev\n",
    "        \"\"\"\n",
    "        \n",
    "        m = dZ.shape[1]\n",
    "        \n",
    "        dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        assert dW.shape == self.W.shape\n",
    "        assert db.shape == self.b.shape\n",
    "        assert dA_prev.shape == A_prev.shape\n",
    "        \n",
    "        return dW, db, dA_prev\n",
    "\n",
    "        \n",
    "    def _act_bwd(self, dA):\n",
    "        \"\"\"\n",
    "        Private method for internal use only.\n",
    "        \n",
    "        Calculates partial derivatives of loss with respect to this layer's linear output (dZ),\n",
    "            given partial derivative of loss with respect to final output (dA)\n",
    "        \n",
    "        Arguments:\n",
    "        dA = partial derivative of loss with respect to final output of this layer\n",
    "        \n",
    "        Returns:\n",
    "        dZ\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            dZ = Layer._relu_bwd(dA, self.Z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            dZ = Layer._sigmoid_bwd(dA, self.Z)\n",
    "            \n",
    "        assert (dZ.shape == dA.shape == self.Z.shape)\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(Z):\n",
    "        \"\"\"\n",
    "        Private, static method for internal use only (call with Layer._sigmoid)\n",
    "        \n",
    "        Arguments:\n",
    "        Z = linear output of layer\n",
    "        \n",
    "        Returns:\n",
    "        sigmoid function of Z\n",
    "        \"\"\"\n",
    "\n",
    "        return 1 / (1+np.exp(-Z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid_bwd(dA, Z):\n",
    "        \"\"\"\n",
    "        Private, static method for internal use only (call with Layer._sigmoid_bwd).\n",
    "        \n",
    "        Calculates and returns partial derivative of loss with respect to linear output \n",
    "            of layer by taking derivative of sigmoid function and multiplying by partial \n",
    "            derivative of loss with regards to final output of layer (chain rule).\n",
    "        \n",
    "        Arguments:\n",
    "        dA = partial derivative of loss with respect to final output of layer\n",
    "        Z = linear output of layer\n",
    "        \n",
    "        Returns:\n",
    "        dZ = partial derivative of loss with respect to linear output of layer\n",
    "        \"\"\"\n",
    "\n",
    "        s = Layer._sigmoid(Z)\n",
    "        dZ = dA * s * (1-s)\n",
    "        return dZ\n",
    "\n",
    "    @staticmethod\n",
    "    def _relu(Z):\n",
    "        \"\"\"\n",
    "        Private, static method for internal use only (call with Layer._rele)\n",
    "        \n",
    "        Arguments:\n",
    "        Z = linear output of layer\n",
    "        \n",
    "        Returns:\n",
    "        relu function of Z\n",
    "        \"\"\"\n",
    "\n",
    "        return np.maximum(0,Z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu_bwd(dA, Z):\n",
    "        \"\"\"\n",
    "        Private, static method for internal use only (call with Layer._relu_bwd).\n",
    "        \n",
    "        Calculates and returns partial derivative of loss with respect to linear output \n",
    "            of layer by taking derivative of relu function and multiplying by partial \n",
    "            derivative of loss with regards to final output of layer (chain rule).\n",
    "        \n",
    "        Arguments:\n",
    "        dA = partial derivative of loss with respect to final output of layer\n",
    "        Z = linear output of layer\n",
    "        \n",
    "        Returns:\n",
    "        dZ = partial derivative of loss with respect to linear output of layer\n",
    "        \"\"\"\n",
    "\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "        dZ[Z <= 0] = 0\n",
    "        \n",
    "        return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.49994167,  0.50007297,  0.50002416, ...,  0.49997188,\n",
       "         0.50007149,  0.50005186],\n",
       "       [ 0.49994186,  0.50013901,  0.5000181 , ...,  0.49992277,\n",
       "         0.50011083,  0.50009677],\n",
       "       [ 0.49999204,  0.49999912,  0.50000428, ...,  0.50000421,\n",
       "         0.50000328,  0.4999997 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(3,2, 'sigmoid')\n",
    "\n",
    "A_prev = np.random.randn(2,500) * 0.01\n",
    "\n",
    "A = layer.forward_prop(A_prev)\n",
    "print(A.shape)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.69833592e-05,   4.27018956e-05,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   1.73682996e-05,   2.83946889e-05],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   3.49067312e-05, ...,\n",
       "          2.04124472e-04,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  3.81905422e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(3,2, 'relu')\n",
    "\n",
    "A = layer.forward_prop(A_prev)\n",
    "print(A.shape)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    \"\"\"\n",
    "    Implementation of extensible neural network\n",
    "    \n",
    "    Stored parameters:\n",
    "        layers = dictionary of Layer objects where key is equal to layer number\n",
    "            (i.e., self.layers[1] is first layer of network)\n",
    "        L = number of layers\n",
    "        AL = matrix of final output of network, initialized to zero initially.\n",
    "            Updated whenever forward_prop is called\n",
    "    \n",
    "    To use, first initialize, and then run train.  Call predict to get binarized predictions.\n",
    "    \n",
    "    All private methods are for internal use and shouldn't need to be called.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims):\n",
    "        \"\"\"\n",
    "        Initialize network\n",
    "        \n",
    "        Arguments:\n",
    "        layer_dims: list of node sizes of network (e.g., [5,4,1] represents 2 layer network\n",
    "            with 5 inputs, 4 nodes in hidden layer, and 1 node in output layer)\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "\n",
    "        self.L = len(layer_dims) - 1\n",
    "        self.layers = {}\n",
    "        self.AL = None\n",
    "        \n",
    "        for l in range(1,self.L+1):\n",
    "            activation = 'relu'\n",
    "            if l == (self.L): #last layer, use sigmoid activation function\n",
    "                activation = 'sigmoid'\n",
    "\n",
    "            curr_layer = Layer(layer_dims[l],layer_dims[l-1],activation)\n",
    "            assert(curr_layer.W.shape == (layer_dims[l],layer_dims[l-1]))\n",
    "            assert(curr_layer.b.shape == (layer_dims[l],1))\n",
    "            self.layers[l] = curr_layer\n",
    "\n",
    "        assert(len(self.layers) == self.L)\n",
    "\n",
    "    def train(self, X, Y, learning_rate = 0.05, num_iterations = 2000, print_costs = True):\n",
    "        \"\"\"\n",
    "        Trains the network by calling _forward_prop and _backward_prop repeatedly.\n",
    "        \n",
    "        Arguments\n",
    "        X = matrix of inputs\n",
    "        Y = matrix of correct answers\n",
    "        learning_rate\n",
    "        num_iterations = number of times to update parameters\n",
    "        print_costs = if true, then will print out cost every 100 iterations\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(X.shape[1] == Y.shape[1])\n",
    "        assert(X.shape[0] == self.layers[1].W.shape[1] and Y.shape[0] == self.layers[self.L].W.shape[0])\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            self._forward_prop(X)\n",
    "            self._backward_prop(X, Y, learning_rate)\n",
    "            \n",
    "            if (i % 100 == 0) and print_costs:\n",
    "                cost = self.cost(Y)\n",
    "                print(\"Cost after {} iterations: {}\".format(i,cost))\n",
    "                if (cost == np.nan):\n",
    "                    print('yeah!')\n",
    "                    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Produces binarized predictions (0 or 1) given current final output of network\n",
    "        \n",
    "        Returns\n",
    "        Binarized predictions\n",
    "        \"\"\"\n",
    "        Y_hat = np.array(self.AL, copy=True)\n",
    "        Y_hat[Y_hat <= 0.5] = 0\n",
    "        Y_hat[Y_hat > 0.5] = 1\n",
    "        \n",
    "        return Y_hat\n",
    "        \n",
    "    def cost(self, Y):\n",
    "        \"\"\"\n",
    "        Returns cost of network given current output and Y\n",
    "        \n",
    "        Arguments:\n",
    "        Y = correct answers\n",
    "        \n",
    "        Returns:\n",
    "        cost\n",
    "        \"\"\"\n",
    "        \n",
    "        m = Y.shape[1]\n",
    "        cost = - 1/m *  np.sum(Y*np.log(self.AL) + (1-Y)*(np.log(1-self.AL)))\n",
    "        cost = np.squeeze(cost)\n",
    "        assert(cost.shape == ())\n",
    "        \n",
    "        return cost   \n",
    "\n",
    "    def _forward_prop(self,X):\n",
    "        \"\"\"\n",
    "        Private method for internal use.\n",
    "        \n",
    "        Implements forward propagation through each layer of network, and updates final output\n",
    "        \n",
    "        Arguments\n",
    "        X = inputs\n",
    "        \n",
    "        Returns\n",
    "        None (AL is updated internally)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[1]\n",
    "        assert(X.shape[0] == self.layers[1].W.shape[1])\n",
    "        \n",
    "        A_prev = X\n",
    "        \n",
    "        for l in self.layers:\n",
    "            curr_layer = self.layers[l]\n",
    "            A_prev = curr_layer.forward_prop(A_prev)\n",
    "        \n",
    "        self.AL = A_prev\n",
    "    \n",
    "    def _backward_prop(self,X,Y,learning_rate):\n",
    "        \"\"\"\n",
    "        Private method for internal use.\n",
    "        \n",
    "        Implements backward propagation and updates parameters through each layer of network\n",
    "        \n",
    "        Arguments\n",
    "        X = inputs\n",
    "        Y = correct answers\n",
    "        learning_rate\n",
    "        \n",
    "        Returns\n",
    "        None (layer parameters are updated internally)\n",
    "        \"\"\"\n",
    "\n",
    "        dA = -1 * (Y/self.AL - (1-Y)/(1-self.AL))\n",
    "        \n",
    "        for l in reversed(range(1,self.L+1)):\n",
    "            if l != 1:\n",
    "                A_prev = self.layers[l-1].A\n",
    "            else:\n",
    "                A_prev = X\n",
    "            \n",
    "            curr_layer = self.layers[l]\n",
    "\n",
    "            dA = curr_layer.backward_prop(dA, A_prev)\n",
    "            curr_layer.update_params(learning_rate)\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Provides string representation of neural network.  Prints out shape and activation \n",
    "        function of each layer\n",
    "        \"\"\"\n",
    "        \n",
    "        to_return = ''\n",
    "        for l in self.layers:\n",
    "            layer = self.layers[l]\n",
    "            to_return += \"Layer: {}\\n\\tW.shape = {}\\n\\tb.shape = {}\\n\\tactivation function = {}\\n\\n\".format(l,layer.W.shape,layer.b.shape,layer.activation)\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2000)\n",
      "(1, 2000)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X = np.random.randint(1,4,(3,2000))\n",
    "print(X.shape)\n",
    "Y = np.sum(X, axis=0, keepdims=True) > 5\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 1\n",
      "\tW.shape = (5, 3)\n",
      "\tb.shape = (5, 1)\n",
      "\tactivation function = relu\n",
      "\n",
      "Layer: 2\n",
      "\tW.shape = (1, 5)\n",
      "\tb.shape = (1, 1)\n",
      "\tactivation function = sigmoid\n",
      "\n",
      "\n",
      "Cost after 0 iterations: 0.6932194624000588\n",
      "Cost after 100 iterations: 0.6490886505927972\n",
      "Cost after 200 iterations: 0.6134729245725072\n",
      "Cost after 300 iterations: 0.5778732113876883\n",
      "Cost after 400 iterations: 0.5369034915895188\n",
      "Cost after 500 iterations: 0.48948330802703166\n",
      "Cost after 600 iterations: 0.43490673129796587\n",
      "Cost after 700 iterations: 0.3757085803171267\n",
      "Cost after 800 iterations: 0.3223826457578583\n",
      "Cost after 900 iterations: 0.2752828099362122\n",
      "Cost after 1000 iterations: 0.23537657316525576\n",
      "Cost after 1100 iterations: 0.2066740022515392\n",
      "Cost after 1200 iterations: 0.18458331109283438\n",
      "Cost after 1300 iterations: 0.16526103803287845\n",
      "Cost after 1400 iterations: 0.14833599735720596\n",
      "Cost after 1500 iterations: 0.1334914644242054\n",
      "Cost after 1600 iterations: 0.12045545107612288\n",
      "Cost after 1700 iterations: 0.10899381295862737\n",
      "Cost after 1800 iterations: 0.09890414545789228\n",
      "Cost after 1900 iterations: 0.09001083909719604\n",
      "(1, 2000)\n"
     ]
    }
   ],
   "source": [
    "nn = neural_network([3,5,1])\n",
    "print(nn)\n",
    "\n",
    "nn.train(X,Y,num_iterations=2000,learning_rate = 0.05)\n",
    "print(nn.AL.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict()\n",
    "correct = predictions == Y\n",
    "true_predictions = correct[correct == True]\n",
    "true_predictions = true_predictions.reshape(1,true_predictions.shape[0])\n",
    "print(\"Accuracy = {}%\".format(true_predictions.shape[1]/correct.shape[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
